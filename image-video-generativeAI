https://huggingface.co/blog/instruction-tuning-sd


2023 latest resources ->

https://github.com/SkalskiP/courses


Generative AI to power the Next-Gen Document Understanding ğŸ“„ğŸ¤–
Documents and the information they contain are crucial for companies ğŸ’¼. Extracting valuable insights has always been a challenge, but not anymore. Say goodbye to traditional OCR engines and welcome Donutâ€”a state-of-the-art, MIT-licensed Generative AI model that removes the need for OCR and processes your documents directly.

https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_generative-ai-for-document-understanding-activity-7066817527091220480-RZMD?utm_source=share&utm_medium=member_desktop


Youtube of AI video 
https://www.youtube.com/@mreflow


https://www.youtube.com/watch?v=h3AhYJ8YVss
Tools Used:
https://futuretools.link/chatgpt
https://futuretools.link/runway
https://futuretools.link/midjourney
https://futuretools.link/leiapix-com
https://futuretools.link/genmo-ai
https://futuretools.link/kaiber-ai
https://futuretools.link/elevenlabs-io
https://futuretools.link/mubert-com
https://www.blackmagicdesign.com/



Awesome Multimodal LLMs

If you want to find recent multimodal LLM papers, this is a great place to start.
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models



28-June
ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğ¢ğ§ğ  ğ‹ğ‹ğŒ ğ¢ğ§ğŸğğ«ğğ§ğœğ ğ­ğ¢ğ¦ğ ğ°ğ¢ğ­ğ¡ğ¨ğ®ğ­ ğ«ğğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ 
One of the most useful articles I have seen lately. Everyone is talking about using LLM locally but no one is really saying about how to infer in production-realistic time. Well here you are.

NOTE: I have left out some of the points which would require retraining the model, as finetuning with adapters - LoRA - QLoRA

â–ªï¸ Use LitGPT to make things easier. https://lnkd.in/dED4yPbT;
â–ªï¸ Brain 16-bits, or mixed 16 bits precision (or Brain 16 mixed precision). Here you need to consider the memory usage - inference time tradeoff which is impacted by the type of precision you use.
â–ªï¸ Post-training Quantization
â–ªï¸ LLM pruning (LLM-Pruner or Wanda)
â–ªï¸ Batch size dynamic adaptation for batch processing (using vLLM: https://lnkd.in/d45NNH6c or Text Generation Inference: https://lnkd.in/dhBnkyYW)
â–ªï¸ For less memory - more GPUs. For fastest inference, less GPUs.

ğŸ“œhttps://lnkd.in/divhyqVP


