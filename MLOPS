Modsel metrics to measure

ğŸ­. ğ—¦ğ—¼ğ—³ğ˜ğ˜„ğ—®ğ—¿ğ—² - ğ—°ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—² ğ—®ğ—»ğ—± ğ—²ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜ ğ˜‚ğ˜€ğ—²ğ—± ğ˜ğ—¼ ğ˜€ğ—²ğ—¿ğ˜ƒğ—² ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹. 

The metrics tracked were memory usage, compute utilization and prediction latency. If there were ever errors or over-usage of resources, there were alerts set up to notify me and my team via email.

ğŸ®. ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—œğ—»ğ—½ğ˜‚ğ˜ - ğ—¾ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—±ğ—®ğ˜ğ—® ğ—¶ğ—» ğ˜ğ—µğ—² ğ—±ğ—¼ğ˜„ğ—»ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—º ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€. 

The metrics tracked were null rate, outliers, and trend shifts. If there were shifts and performance declined greatly, then it's important to consider re-designing the model

ğŸ¯. ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² - ğ—¼ğ—»ğ—¹ğ—¶ğ—»ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—½ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—²

It's not enough to achieve a high score on an offline dataset (that's Kaggle). What matters more is your model performs just as well in production.

Primarily there were two sets of metrics tracked in production that included the time-series CV scores of the MAPE, MPE, RMSE, and forecast variance.

ğŸ°. ğ—¨ğ˜€ğ—²ğ—¿ ğ—œğ—»ğ˜ğ—²ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» - ğ˜€ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ˜ğ—¶ğ—ºğ—², ğ˜ğ—¿ğ—²ğ—»ğ—± ğ˜€ğ—µğ—¶ğ—³ğ˜

The final deliverable was in the form of a dashboard. At the end of the day, the model is designed to serve a user. It's vital to track the usage. From the dashboard, session time and trend shifts were tracked.
